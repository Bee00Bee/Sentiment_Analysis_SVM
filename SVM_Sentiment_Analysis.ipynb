{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68955367",
   "metadata": {},
   "source": [
    "# Logistic Regression in Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17d768",
   "metadata": {},
   "source": [
    "In this work we want to explore the usage of Logistic Regression (LR) in NLP. The dataset is available in [NLTK](https://www.nltk.org/howto/twitter.html) library. The library includes the positive and negative tweets. We want to predict the sentiment (positive and negative opinions) of each tweets. The machine learning approach here is a supervised learning approach since we use the 80% to train the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5047b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                         \n",
    "from os import getcwd\n",
    "import pandas as pd                 \n",
    "from nltk.corpus import twitter_samples \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec9a57",
   "metadata": {},
   "source": [
    "Apart from the python libraries we need to import two functions: process_tweet and build_freqs. The process_tweet function cleans up the tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0202e",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "208dfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_twts = twitter_samples.strings('positive_tweets.json')\n",
    "neg_twts = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "twts = pos_twts + neg_twts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb14ff5",
   "metadata": {},
   "source": [
    "The tweets which have the positive sentiment are labeled as 1 and negative tweets are labeled as 0. The labels is created down here as the numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "919151f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.append( np.ones((len(pos_twts),1)), np.zeros((len(neg_twts),1)), axis = 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8d057",
   "metadata": {},
   "source": [
    "## Cleaning up the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bb0c7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \n",
    "    # Instantiate stemming class\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    tweet = re.sub(r'^RT[\\s]+','', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'@', '', tweet)\n",
    "    tweet = re.sub(r'@', '', tweet)\n",
    "    tweet = re.sub(r'\\$\\w*','', tweet)\n",
    "    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    twt_tokened = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    clean_twt = []\n",
    "    \n",
    "    for word in twt_tokened:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            clean_twt.append(word)\n",
    "    \n",
    "    stemed_twt = []\n",
    "    for word in clean_twt:\n",
    "        word_stemmed = stemmer.stem(word)\n",
    "        stemed_twt.append(word_stemmed)\n",
    "    \n",
    "    \n",
    "    return stemed_twt\n",
    "\n",
    "# create a empty list for clean tweets\n",
    "clean_twts_lst = []\n",
    "\n",
    "for i in twts:\n",
    "    clean_twts_lst.append(process_tweet(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bdb20b",
   "metadata": {},
   "source": [
    "The tweets after the process need are tokenized. So we need to put them back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7b4da42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = \" \"\n",
    "clean_twts_lst2 = []\n",
    "\n",
    "for i in clean_twts_lst:\n",
    "    clean = \" \"\n",
    "    clean = clean.join(i)\n",
    "    clean_twts_lst2.append(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bd723",
   "metadata": {},
   "source": [
    "The tweets are cleaned up and stored in **clean_twts_lst2**. The next step is to combine the strings of tweets are their corresponding 1 and 0 labels. \n",
    "There are 10,000 tweets and 10,000 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2b0ea554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the tweets is:  10000 , which is the same as the length of labels. The length of labels is:  10000\n"
     ]
    }
   ],
   "source": [
    "print('The length of the tweets is: ', len(clean_twts_lst2), ', which is the same \\\n",
    "as the length of labels. The length of labels is: ', len(labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1829d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels arrays to a list\n",
    "\n",
    "# labels list\n",
    "labels2 = []\n",
    "\n",
    "for i in labels:\n",
    "    labels2.append(i[0])\n",
    "    \n",
    "    \n",
    "# create the columns of a dataframe\n",
    "columns = {'Tweets':clean_twts_lst2, 'Label': labels2}\n",
    "\n",
    "# This is the dataframe which has the tweets and their corresponding \n",
    "#labels\n",
    "\n",
    "df_twts_lbls = pd.DataFrame(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "947949c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>followfriday france_int pkuchli 57 milipol_par...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lamb 2ja hey jame odd :/ pleas call contact ce...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>despiteoffici listen last night :) bleed amaz ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97side congrat :)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaah yipppi accnt verifi rqst succeed got bl...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>wanna chang avi usanel :(</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>puppi broke foot :(</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>where' jaebum babi pictur :(</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>mr ahmad maslan cook :(</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>eawoman hull support expect misser week :-(</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweets  Label\n",
       "0     followfriday france_int pkuchli 57 milipol_par...    1.0\n",
       "1     lamb 2ja hey jame odd :/ pleas call contact ce...    1.0\n",
       "2     despiteoffici listen last night :) bleed amaz ...    1.0\n",
       "3                                     97side congrat :)    1.0\n",
       "4     yeaaah yipppi accnt verifi rqst succeed got bl...    1.0\n",
       "...                                                 ...    ...\n",
       "9995                          wanna chang avi usanel :(    0.0\n",
       "9996                                puppi broke foot :(    0.0\n",
       "9997                       where' jaebum babi pictur :(    0.0\n",
       "9998                            mr ahmad maslan cook :(    0.0\n",
       "9999        eawoman hull support expect misser week :-(    0.0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twts_lbls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8df20",
   "metadata": {},
   "source": [
    "## Make the data ready for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d0f69",
   "metadata": {},
   "source": [
    "Train test split the data. The x data is the strings of tweets and the y data is the label. The steps are here:\n",
    "\n",
    "    1- Train & test split the data\n",
    "    2- Vectorize the strings of the tweets.\n",
    "    3- Fit and evaluate the model with different solvers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257e358",
   "metadata": {},
   "source": [
    "### 1- Train & test split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bc83815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train:  (8000,) shape of y_train:  (8000,) \n",
      " shape of x_test:  (2000,) shape of y_test:  (2000,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_twts_lbls['Tweets'], df_twts_lbls['Label'],\n",
    "                                                    test_size = 0.2, random_state=40)\n",
    "\n",
    "print('shape of x_train: ', x_train.shape, 'shape of y_train: ', y_train.shape,'\\n',\n",
    "     'shape of x_test: ', x_test.shape, 'shape of y_test: ', y_test.shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e21ad6",
   "metadata": {},
   "source": [
    "### 2- Vectorize the strings of the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502709d",
   "metadata": {},
   "source": [
    "Now we use the TfidfVectorizer to vectorize the strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ac22d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing the tweets....\n",
      "\n",
      "The shape of the x_test:  (2000,)  and the shape of the x_train is:  (8000,) \n",
      "\n",
      "The shape of the vectorized x_train is:  (8000, 14607)  and the shape of the vectorized x-test is:  (2000, 14607)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing the tweets....\\n\")\n",
    "\n",
    "vectorizer= TfidfVectorizer()\n",
    "tf_x_train = vectorizer.fit_transform(x_train)\n",
    "tf_x_test = vectorizer.transform(x_test)\n",
    "\n",
    "print('The shape of the x_test: ', x_test.shape, ' and the shape of the x_train is: ', x_train.shape,'\\n')\n",
    "\n",
    "print('The shape of the vectorized x_train is: ', tf_x_train.shape,' and the shape of the vectorized x-test is: ', \n",
    "     tf_x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f979d03",
   "metadata": {},
   "source": [
    "### 3- Fit and evaluate the model with different solvers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ff027",
   "metadata": {},
   "source": [
    "There are different solvers which is accessible in the **SVM**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "267897a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.0': {'precision': 0.7521697203471552,\n",
       "  'recall': 0.7507218479307026,\n",
       "  'f1-score': 0.7514450867052023,\n",
       "  'support': 1039},\n",
       " '1.0': {'precision': 0.731048805815161,\n",
       "  'recall': 0.7325702393340271,\n",
       "  'f1-score': 0.7318087318087318,\n",
       "  'support': 961},\n",
       " 'accuracy': 0.742,\n",
       " 'macro avg': {'precision': 0.7416092630811582,\n",
       "  'recall': 0.7416460436323649,\n",
       "  'f1-score': 0.7416269092569671,\n",
       "  'support': 2000},\n",
       " 'weighted avg': {'precision': 0.7420211209145321,\n",
       "  'recall': 0.742,\n",
       "  'f1-score': 0.7420098181774484,\n",
       "  'support': 2000}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0)\n",
    "\n",
    "clf.fit(tf_x_train, y_train)\n",
    "y_pred = clf.predict(tf_x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report=classification_report(y_test, y_pred,output_dict=True)\n",
    "\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
